[
  {
    "title": "Abstract of Lab 4B: What's the score?",
    "body": "This lab, Lab 4B: What's the score?, focuses on measuring the accuracy of predictions made by models. It builds upon previous knowledge of using one variable to predict another. The lab guides users through plotting data, creating prediction functions, calculating residuals (the difference between actual and predicted values), and evaluating model accuracy. Initially, summing residuals is explored but found problematic due to the cancellation of positive and negative errors. The lab then introduces Mean Squared Error (MSE) as a more robust measure of accuracy by squaring residuals to ensure positivity before averaging. Finally, it contrasts user-defined lines with \"regression lines\" (or \"linear models\") generated using the `lm()` function in R, demonstrating that `lm()` finds the line that minimizes MSE, thus providing the best possible fit."
  },
  {
    "title": "Lab 4B Introduction and Previous Concepts",
    "body": "Lab 4B, titled 'What's the score?', is designed to teach how to measure the accuracy of predictions. In the previous lab, participants learned to make predictions about one variable using information from another. This lab extends that concept by focusing on quantifying how accurate those predictions are. This evaluation of accuracy is crucial for understanding how well a model performs and will be used later to compare different models and identify the one that makes the best predictions. The lab involves following along with slides, completing questions in blue text on the computer, and answering red text questions in a journal."
  },
  {
    "title": "Predicting Height with a Line",
    "body": "In this section of Lab 4B, participants are instructed to load the 'arm_span' dataset again. The first computer-based task involves creating an `xyplot` with 'height' on the y-axis and 'armspan' on the x-axis. They should then use `add_line()` or `get_line()` to graph a line that they believe fits the data well. Following this, participants will create a function to predict 'height' based on 'armspan'. This function requires filling in the blanks for the slope and intercept of their chosen line: `predict_height <- function(armspan) { ____ * armspan + ____ }`."
  },
  {
    "title": "Creating Predictions and Adding to Data",
    "body": "Participants in Lab 4B will now incorporate their predictions into the 'arm_span' dataset. This involves using the `mutate()` function to add a new column named `predicted_height`. The values in this column will be generated by applying the `predict_height` function (defined previously with user-chosen slope and intercept) to the 'armspan' values. The code structure for this is: `____ <- mutate(____, predicted_height = ____(____))`. After making predictions, the next step is to determine how accurate these predictions are by comparing them to the actual 'height' values and summarizing this comparison into a single accuracy metric."
  },
  {
    "title": "Understanding Residuals",
    "body": "A core concept introduced in Lab 4B is the *residual*. A residual is defined as the difference between the actual value of a variable and its predicted value. To implement this, participants will add a column of residuals to the 'arm_span' dataframe using the `mutate()` function. The code required is: `____ <- mutate(____, residual = ____ - ____)`. This new column will represent the error of each prediction. Participants are then asked to answer two journal questions: (5) What do the residuals measure? and (8) Why is adding positive and negative errors together problematic for assessing prediction accuracy? The latter question addresses the issue of errors canceling each other out."
  },
  {
    "title": "Evaluating Accuracy with Sum of Residuals",
    "body": "In Lab 4B, one approach explored to measure model accuracy is by summing the residuals. Participants are instructed to calculate this sum using the `summarize()` function: `summarize(____, sum(____))`. The hint provided explains that `summarize()` takes a dataframe as the first argument and an action on a column (like `sum()`) as the second, typically resulting in a single summary number. Following this calculation, participants must answer journal question (7): Describe and interpret, in words, what the output of your accuracy summary means. This exercise highlights the limitations of simply summing residuals, as positive and negative errors can cancel out."
  },
  {
    "title": "Mean Squared Error (MSE) Calculation",
    "body": "Lab 4B addresses the problem of positive and negative residuals canceling each other out by introducing the *Mean Squared Error (MSE)*. The lab explains that squaring the residuals ensures all errors are positive before averaging. The MSE is calculated by squaring all residuals and then finding the mean of these squared values. Participants are tasked with calculating the MSE for their self-defined line using the following R code structure: `summarize(____, mean((____))^2)`. After calculating their MSE, they will compare it with a neighbor's result and discuss whose line was more accurate and why, based on their respective MSE values (Journal Question 10)."
  },
  {
    "title": "Introduction to Regression Lines (Linear Models)",
    "body": "The lab points out that different students will likely draw slightly different lines to fit the data, leading to variations in predictions. To standardize this process and avoid subjective line fitting, data scientists use *regression lines*, also known as *linear models*. A regression line is defined as the line that connects the mean 'height' of people with similar 'armspan' values. Participants will use the `lm()` function (which stands for *linear model*) to create this best-fit line. The code requires filling in the variables for the model: `best_fit <- lm(____ ~ ____, data = arm_span)`."
  },
  {
    "title": "Plotting and Using Regression Lines",
    "body": "After creating the regression line using `lm()` in Lab 4B, participants should type `best_fit` into the R console to view the line's slope and intercept. They will then recreate the scatterplot of 'armspan' vs. 'height' and add the calculated regression line using `add_line()`. This requires filling in the previously obtained intercept and slope values: `add_line(intercept = ____, slope = ____)`. This visually compares the automatically generated best-fit line with the data points."
  },
  {
    "title": "Predicting with Linear Models in R",
    "body": "Lab 4B explains that making predictions using models R is familiar with, like those generated by `lm()`, is simpler than with self-defined lines. Participants will use the `predict()` function in conjunction with `mutate()` to add a new `predicted_height` column to their dataset, based on the `best_fit` linear model. The code structure is: `____ <- mutate(____, predicted_height = predict(____))`. The `predict()` function efficiently takes a linear model object as input and outputs the corresponding predictions for the data."
  },
  {
    "title": "Comparing Model Performance using MSE",
    "body": "The final section of Lab 4B focuses on comparing the performance of different linear models based on their Mean Squared Error (MSE). The lab reiterates that the `lm()` function finds the line that *minimizes* the MSE, making it the optimal line. Participants are asked to calculate the MSE for the predictions generated by the `lm()` model (Journal Question 14). Subsequently, they will compare this MSE value to the MSE they calculated earlier for their own manually drawn line (Journal Question 15). They will also discuss with neighbors whether any of their manually fitted lines achieved a lower MSE than the `lm()` line (Journal Question 16), reinforcing the concept that `lm()` provides the best possible linear fit."
  }
]