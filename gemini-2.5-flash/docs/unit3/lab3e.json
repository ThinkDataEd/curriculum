[
  {
    "title": "Abstract: Comprehensive Overview of Unit 3 Lab 3E - Scraping Web Data",
    "body": "Unit 3 Lab 3E focuses on the fundamental concepts and practical application of web data scraping using R. This lab introduces students to the internet as a vast data source and the automated process of gathering information, known as web scraping. It addresses the inherent challenges due to diverse website data structures. The lab is structured into two main steps: first, gathering raw web information, and second, cleaning and transforming it into a usable data frame, which serves as a prerequisite for Unit 3 Lab 3F. Key topics include understanding HTML (HyperText Markup Language) as the foundational code for websites, identifying and interpreting core HTML tags like <TABLE>, <TR>, <TH>, and <TD> which define tabular data structures. Students will learn to programmatically access and extract data from a specified URL, specifically https://labs.thinkdataed.org/extras/webdata/mountains.html, using R's readHTMLTable() function. The lab also covers methods for handling scenarios where multiple tables are present on a single webpage, employing functions like length() to enumerate tables and the which argument within readHTMLTable() to target specific data tables. Finally, the lab emphasizes data persistence by instructing participants to save the scraped data in an .Rda file format and perform initial exploratory data analysis, such as calculating descriptive statistics (mean and standard deviation of elev_ft) and aggregations (identifying the state with the most mountains). The instructions guide users to follow slides, answer blue questions on a computer, and red questions in a journal, fostering both hands-on programming and conceptual understanding."
  },
  {
    "title": "Unit 3 Lab 3E: Foundational Concepts of Web Data Scraping and Objectives",
    "body": "Unit 3 Lab 3E, titled 'Scraping web data,' serves as an introductory module for understanding how to extract information from the internet programmatically. The lab emphasizes that the web contains enormous quantities of information that can be systematically collected using computers, a process termed 'scraping web data.' Acknowledging that different websites structure and display their data uniquely, the lab highlights the initial challenges associated with this task. The overarching objective of Unit 3 Lab 3E is broken down into two critical steps: first, to successfully gather information from a web source, and second, to clean this acquired data and convert it into a well-structured data frame suitable for further analysis. This processed data will then be directly utilized in Unit 3 Lab 3F. Participants are guided to follow along with provided slides, completing practical questions marked in blue on their computers, and reflecting on conceptual questions marked in red in their journals. This structured approach ensures both hands-on implementation and a deeper comprehension of web scraping methodologies."
  },
  {
    "title": "Exploring the Web as a Data Source and First Scraping Steps in Unit 3 Lab 3E",
    "body": "In Unit 3 Lab 3E, the internet is presented as an invaluable repository of data, which can be harnessed through the automated process of web scraping. This technique involves using computer programs to extract information, but it requires careful handling due to the varied ways websites store and present their data. The lab initiates the practical aspect of web scraping by directing users to a specific target website: https://labs.thinkdataed.org/extras/webdata/mountains.html. Students are prompted to copy and paste this link into a web browser to visually inspect the data intended for scraping. Following this initial view, participants are asked to describe the nature of the data found on the website and formulate three analytical questions they would be interested in answering. These initial steps (red questions 1 and 2) are crucial for understanding the data's context and for setting the stage for subsequent data gathering and cleaning, which will prepare the data for use in Unit 3 Lab 3F."
  },
  {
    "title": "HTML Fundamentals: The Language Behind Web Data in Unit 3 Lab 3E",
    "body": "Unit 3 Lab 3E delves into HTML (HyperText Markup Language), which is the foundational code used to render virtually every webpage encountered on the internet. Understanding HTML is critical for effective web scraping, as it defines the structure and content of web documents. The lab provides a concrete example by displaying the HTML code responsible for rendering the first two rows of the mountain data table from the target website (mountains.html). Students are then challenged with conceptual questions (red questions 3 and 4): to identify how a data table structured in HTML differs from a data table typically viewed in statistical software like R (e.g., using the View() function). Furthermore, they are asked to infer the meaning and function of key HTML tags, specifically <TABLE>, <TR>, <TH>, and <TD>, and explain how these tags are collectively used by HTML to display tabular data. This segment aims to build a solid understanding of the underlying web structure before proceeding with programmatic data extraction."
  },
  {
    "title": "Decoding HTML Tags: Structuring Tables for Web Scraping in Unit 3 Lab 3E",
    "body": "In Unit 3 Lab 3E, a detailed examination of HTML (HyperText Markup Language) is crucial for effective web data extraction. HTML serves as the backbone for displaying all web content, and understanding its tag system is paramount. The lab specifically highlights the HTML structure for tabular data, as exemplified by the provided snippet from the mountains.html site. It explains the roles of various tags: the <TABLE> tag delineates the entire data table, <TR> (table row) defines each individual row within the table, <TH> (table header) specifies the column headers for the table, and <TD> (table data) encloses the actual data points within each cell of the table. By presenting this hierarchical structure, the lab illustrates how HTML precisely organizes and displays tabular information. Participants are asked to consider how this raw HTML table format contrasts with the more familiar, processed data tables seen in R, for instance, when using the View() function, thus deepening their comprehension of data representation across different platforms."
  },
  {
    "title": "Executing the Initial R Code for Web Scraping in Unit 3 Lab 3E",
    "body": "Following the foundational understanding of HTML, Unit 3 Lab 3E transitions to the practical implementation of web scraping using R. The first step in the 'Get to scraping!' section directs users to revisit the mountains.html website, the source of the data of interest. The URL address of this site is then to be assigned to an R variable named data_url. This variable will serve as the reference point for the scraping function. The core of this initial scraping phase involves using the readHTMLTable() function in R. Participants are guided to fill in the blank in the code tables <- readHTMLTable(____) to instruct R to scrape *every* web table found on the specified data_url. This command is the gateway to programmatically fetching data from the web, laying the groundwork for identifying, cleaning, and analyzing the collected information, ultimately preparing it for subsequent tasks in Unit 3 Lab 3F."
  },
  {
    "title": "Identifying and Selecting Specific Data Tables from Web Scrapes in Unit 3 Lab 3E",
    "body": "Unit 3 Lab 3E highlights a common scenario in web scraping: the readHTMLTable() function often extracts *all* tables present on a given web URL. As illustrated by examples like wikipedia.org, a single webpage can host multiple distinct tables, not all of which may be relevant to the user's specific data needs. Therefore, a crucial step after the initial scrape is to ascertain which of the retrieved tables contains the desired data. To facilitate this, the lab instructs participants to utilize the length() function in R. By writing and executing code such as length(tables), students can determine the total number of data tables that were successfully scraped into their tables object. This quantitative assessment is essential for navigating the scraped content and pinpointing the exact table required for analysis, setting the stage for more precise data extraction in the subsequent steps of Unit 3 Lab 3E."
  },
  {
    "title": "Refining Web Scraping with `which` Argument for Precise Data Retrieval in Unit 3 Lab 3E",
    "body": "Once the total number of tables scraped from a web page has been determined, Unit 3 Lab 3E progresses to the precise extraction of the target data. The lab explains that the readHTMLTable() function in R can be refined using the which argument, allowing users to specify exactly which table they wish to extract when multiple tables are present. This is a critical step for isolating the relevant data from potentially many irrelevant tables. Participants are instructed to write and execute R code that re-scrapes the data from the web, but this time employing the which argument. The value provided to which should be an integer corresponding to the sequential position of the desired table among all scraped tables. The extracted, single table of mountain data is then to be assigned the descriptive name mtns in R. This focused approach ensures that only the necessary information is retained for further processing and analysis in Unit 3 Lab 3E and subsequent activities, including Unit 3 Lab 3F."
  },
  {
    "title": "Saving Scraped Data and Performing First Analytical Queries in Unit 3 Lab 3E",
    "body": "After successfully scraping and isolating the desired data, Unit 3 Lab 3E emphasizes the importance of data persistence and immediate utility. The lab directs participants to save their newly acquired mtns data. This is achieved by filling in the blanks of the R command: save(____, file = \"____.Rda\"). This step ensures that the scraped data is preserved in an .Rda file, making it readily available for future use without needing to re-scrape the web. Beyond saving, the lab prompts students to immediately engage with the mtns dataset through exploratory data analysis. Specifically, two key analytical questions are posed: (1) calculating the mean and standard deviation of the elev_ft variable, and (2) identifying which state in the dataset contains the highest number of mountains. These questions serve to validate the scraping process and initiate fundamental data insights, setting the foundation for more advanced analysis in Unit 3 Lab 3F."
  },
  {
    "title": "Post-Scraping Workflow: Data Storage, Verification, and Preliminary Analysis in Unit 3 Lab 3E",
    "body": "The culmination of the web scraping efforts in Unit 3 Lab 3E involves solidifying the acquired data and performing initial checks. A critical step is to save the mtns data frame, which now contains the cleaned and isolated mountain information, into a persistent file format. The save() function in R is used for this purpose, with the instruction to specify the data object (e.g., mtns) and a file path ending in .Rda (e.g., save(mtns, file = \"mountains_data.Rda\")). This action ensures that the data is stored locally and can be reloaded later for continued work or for its intended use in Unit 3 Lab 3F. To confirm the integrity and readiness of the scraped data, participants are challenged with basic analytical tasks: determining the average (mean) and variability (standard deviation) of mountain elevations in feet (elev_ft), and identifying the state that features the most mountains within the collected dataset. These tasks serve as a final verification of the scraping process and provide immediate insights from the extracted web data."
  }
]