[
  {
    "title": "Abstract: Building and Evaluating Decision Trees in Unit 4 Lesson 17",
    "body": "Unit 4 Lesson 17, titled \"Grow Your Own Decision Tree,\" guides students through the process of constructing classification trees and evaluating their predictive accuracy. The primary objective is for students to develop their own decision trees using provided training data, specifically player statistics, and then rigorously test the effectiveness of these trees against new, unseen test data. Key concepts introduced include the misclassification rate (MCR), which quantifies the proportion of incorrect predictions made by a classification tree, distinguishing it from metrics like Mean Squared Error (MSE) or Mean Absolute Error (MAE) used for linear models. Students utilize the \"Make Your Own Decision Tree\" handout (LMR_U4_L17) for this hands-on activity, optionally using RStudio code for data manipulation. The lesson emphasizes the critical roles of training data in model building and test data in preventing overfitting and ensuring generalizability. It also prepares students for future automated tree creation and MCR calculation using RStudio in the upcoming Unit 4 Lab 4G, illustrating these concepts with a Titanic dataset example. Discussion questions at the lesson's conclusion reinforce understanding of decision trees as predictive models and the importance of data splitting."
  },
  {
    "title": "Objectives and Materials for Creating Decision Trees in Unit 4 Lesson 17",
    "body": "In Unit 4 Lesson 17, \"Grow Your Own Decision Tree,\" the main objective is for students to actively construct their own decision trees. This process begins with students utilizing training data, which in this lesson consists of player statistics previously encountered. Following the creation phase, students will critically evaluate how well their self-designed decision trees perform when applied to new, unseen test data. The essential material for this hands-on activity is the \"Make Your Own Decision Tree\" handout, referenced as LMR_U4_L17, which provides structured guidance and the necessary data. An optional component allows students to use specific RStudio code to recreate and manipulate the training data, enhancing their practical experience with data preparation for model building. This setup ensures that students not only learn the theoretical aspects of decision trees but also gain practical experience in building and validating them."
  },
  {
    "title": "Misclassification Rate (MCR) and Data Roles in Decision Tree Evaluation (Unit 4 Lesson 17)",
    "body": "Unit 4 Lesson 17 introduces students to crucial vocabulary and concepts for evaluating classification models, specifically decision trees. The primary metric for assessing classification trees is the **misclassification rate (MCR)**, defined as the proportion of observations that were predicted to be in one category but actually belonged to another. This contrasts with metrics like Mean Squared Error (MSE) or Mean Absolute Error (MAE), typically used for linear models. The lesson also distinguishes between **training data** and **test data**. Training data, typically 75-85% of the original dataset, is used to build or \"train\" the model. Conversely, test data, comprising the remaining 15-25%, is reserved to evaluate the model's performance on unseen observations, ensuring the model is not overfitted and can generalize well. The usefulness of decision trees, as an essential concept highlighted in the lesson, is directly determined by comparing their misclassification rates."
  },
  {
    "title": "Assessing Model Predictions: From Linear Models to Classification Trees in Unit 4 Lesson 17",
    "body": "Unit 4 Lesson 17 begins by drawing a parallel between assessing linear models and classification trees. Students recall that linear models, which predict numerical outcomes, are typically evaluated using metrics such as Mean Squared Error (MSE) and Mean Absolute Error (MAE). For classification trees, which predict categorical outcomes, a different but analogous method is employed: the **misclassification rate (MCR)**. The MCR quantifies the proportion of instances where the tree's prediction for an observation's category did not match its actual category. To solidify this concept, the lesson prompts students to revisit their tallied decision trees and classification tables from previous activities. For example, if all activity player stats cards were used, the MCR for Round 1 might be 9/15 (0.60) and for Round 2, 2/15 (0.13), demonstrating how these proportions of incorrect classifications directly represent the MCR. This review establishes a foundational understanding of MCR before students create their own trees."
  },
  {
    "title": "Building Decision Trees with Training Data and Introducing Test Data in Unit 4 Lesson 17",
    "body": "A core activity in Unit 4 Lesson 17 involves students actively creating their own classification trees. They are provided with a specific dataset, referred to as the **training data**, which includes player statistics such as Team, Player, Height, Weight, Age, and League (NFL or USMNT). Students receive the \"Make Your Own Decision Tree\" handout (LMR_U4_L17), guiding them to develop a series of yes/no questions to classify each player into their correct league. This initial phase focuses on building a model that accurately predicts outcomes based on *known* data. The lesson emphasizes the importance of using this training data to construct the tree. After building their trees, students are introduced to the concept of **test data**, which consists of new, \"mystery\" players. They are tasked with using their newly created decision trees to classify these mystery players, setting the stage for evaluating the tree's predictive accuracy on unseen observations."
  },
  {
    "title": "Evaluating Decision Tree Performance with Test Data and Understanding Overfitting in Unit 4 Lesson 17",
    "body": "After students construct their decision trees using the provided **training data** in Unit 4 Lesson 17, the crucial next step is to evaluate their trees' performance. This evaluation is done by applying the trees to **test data**, which comprises five \"mystery players\" whose actual league classifications are initially unknown to the students. Students record their tree's predictions for these mystery players using the \"Make Your Own Decision Tree\" handout (LMR_U4_L17). This process directly addresses questions about the generalizability of their trees: \"Will you be able to classify other players from a new dataset correctly?\" and \"Do you think this classification tree is too specific to the training data?\" The exercise highlights the risk of overfitting, where a model performs exceptionally well on training data but poorly on new, unseen test data. Comparing predictions against the correct league classifications for the mystery players then allows students to quantitatively assess the accuracy of their decision trees and understand the practical implications of a good or bad misclassification rate (MCR)."
  },
  {
    "title": "Preparing for RStudio: Automated Decision Tree Creation and MCR Calculation in Unit 4 Lab 4G",
    "body": "Unit 4 Lesson 17 concludes by preparing students for more advanced methods of creating and evaluating decision trees. The lesson acknowledges that manually creating classification trees becomes exceedingly difficult and time-consuming when dealing with larger, more complex datasets. Therefore, data scientists commonly rely on specialized software for this task. Students are informed that they will transition to using RStudio in the upcoming **Unit 4 Lab 4G: Growing Trees** to automate the creation of tree models. RStudio not only builds the trees but also calculates the **misclassification rate (MCR)** efficiently. The lesson provides a \"front-load\" explanation of how RStudio presents MCR calculations, using an example from the Titanic dataset. This preview helps students understand the RStudio output, which displays ratios within leaf nodes indicating the number of misclassifications relative to the total observations in that node, even before they start Unit 4 Lab 4G."
  },
  {
    "title": "Interpreting Misclassification Rate (MCR) in RStudio with the Titanic Data Example (Unit 4 Lesson 17)",
    "body": "To bridge the gap between manual and automated decision tree analysis, Unit 4 Lesson 17 provides a detailed example of how RStudio calculates and presents the **misclassification rate (MCR)**, specifically referencing the Titanic dataset. This example involves a total of 1000 observations (people) from the Titanic data. The classification tree attempts to predict survival based on sex, investigating the hypothesis of women being prioritized on lifeboats. In RStudio's output, each \"leaf node\" of the classification tree shows a ratio, such as \"125/641.\" The denominator (e.g., 641) indicates the total number of observations classified into that specific node (e.g., classified as not surviving because they were male). The numerator (e.g., 125) represents the number of **misclassifications** within that node (e.g., 125 males who *actually survived* but were predicted not to). Students learn to calculate the overall MCR by summing all numerators (total misclassifications) and dividing by the total number of observations (sum of all denominators, e.g., (125+96)/(641+359) = 221/1000 or 0.221). This provides a concrete understanding of MCR computation in a real-world scenario."
  },
  {
    "title": "Homework and Next Steps: Reflecting on Decision Trees, Linear Models, and Data Roles (Unit 4 Lesson 17)",
    "body": "Following the hands-on decision tree creation and evaluation in Unit 4 Lesson 17, students are assigned homework designed to deepen their conceptual understanding. The discussion questions prompt them to compare and contrast decision trees (specifically CART models) with linear models, considering similarities like making predictions and differences such as the types of data they handle (categorical vs. numerical outcomes). Another question asks why a decision tree is considered a \"model,\" encouraging students to articulate its function in representing relationships between variables. Crucially, students are asked to describe the distinct roles of **training data** and **test data** in building and validating a classification tree. This reinforces the idea that training data is used for model construction, while test data is essential for assessing generalizability and preventing overfitting. This preparation is vital for the upcoming **Unit 4 Lab 4G: Growing Trees**, where students will apply these concepts in RStudio to create tree models and calculate misclassification rates, further solidifying their understanding before proceeding to Unit 4 Lesson 18."
  },
  {
    "title": "Essential Concepts: Determining Decision Tree Usefulness via Misclassification Rate (MCR) (Unit 4 Lesson 17)",
    "body": "Unit 4 Lesson 17, \"Grow Your Own Decision Tree,\" reinforces a fundamental concept: the utility of decision trees is primarily determined by their effectiveness in classification, specifically through the **misclassification rate (MCR)**. The lesson emphasizes that by comparing the number of misclassifications made by different decision trees, one can quantitatively assess their predictive power. This MCR is defined as the proportion of observations that were incorrectly categorized â€“ predicted to be in one class but actually belonging to another. This metric stands as the key assessment tool for classification trees, much like Mean Squared Error (MSE) or Mean Absolute Error (MAE) are for linear models. Students engage in calculating MCR from previous activities, such as identifying incorrect classifications from Round 1 and Round 2 of a player data exercise, where an MCR of 0.60 for Round 1 and 0.13 for Round 2 might have been observed, directly connecting theoretical definitions to practical examples. This ensures a clear understanding of what makes a decision tree \"useful.\""
  },
  {
    "title": "The Practical Challenges of Manual Tree Creation and the Necessity of Software Tools (Unit 4 Lesson 17)",
    "body": "Unit 4 Lesson 17 highlights the practical limitations of manually creating decision trees. While students spend time building their own classification trees using the \"Make Your Own Decision Tree\" handout (LMR_U4_L17) and **training data** (player statistics), the lesson explicitly states that this becomes impractical with larger datasets. The complexity of constructing effective trees by hand increases significantly with more data points and variables. Consequently, data scientists rely heavily on software tools to automate this process. This realization serves as a crucial transition point, preparing students for the upcoming **Unit 4 Lab 4G: Growing Trees**. In this lab, they will learn to utilize RStudio to efficiently generate tree models that make robust predictions without requiring an excessive number of branches. The lesson even previews how RStudio calculates and presents the **misclassification rate (MCR)**, using an example from the Titanic dataset, showcasing the ratios within leaf nodes that indicate correct and incorrect classifications, thus setting the stage for more advanced, software-aided analysis."
  },
  {
    "title": "Detailed Training Data and Handout for Self-Constructed Decision Trees in Unit 4 Lesson 17",
    "body": "In Unit 4 Lesson 17, a pivotal part of the learning experience involves students independently creating their own decision trees. To facilitate this, a specific dataset is provided, serving as the **training data**. This data details 15 players with attributes such as Team, Player Name, Height (inches), Weight (pounds), Age, and their respective League (NFL or USMNT). Examples include players like Desmond Ridder (NFL), Matt Miazga (USMNT), and Brock Purdy (NFL), providing a diverse set of characteristics for classification. Students are equipped with the \"Make Your Own Decision Tree\" handout (LMR_U4_L17), which contains this training data and provides instructions. The handout directs them to formulate a series of yes/no questions to effectively classify each player into their correct league. This hands-on exercise with concrete player data allows students to directly apply the theoretical principles of decision tree construction, preparing them to then test their trees' accuracy against new observations. Optional RStudio code is available to enable students to recreate and manipulate this training data digitally if desired."
  }
]