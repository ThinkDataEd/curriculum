[
  {
    "title": "Lab 4G: Growing Trees - An Introduction to Classification and Regression Tree Models",
    "body": "Unit 4 Lab 4G introduces the concept of decision tree models as an alternative to linear models for making predictions. The lab focuses primarily on *classification trees*, which predict categorical outcomes, using the `titanic` dataset. Students learn to create, visualize, and interpret these trees using the `tree()` and `treeplot()` functions. Key activities include building initial trees (`tree1`, `tree2`) to predict passenger survival based on variables like `sex`, `age`, `class`, and `embarked`. The lab also explores controlling tree complexity through parameters like `cp` (complexity parameter) and `minsplit` (minimum observations for a split), leading to the creation of `tree3` and `tree4`. A significant part of the lab is dedicated to evaluating model performance using *cross-validation* and the *misclassification rate* (MCR) for categorical predictions, contrasting it with the *mean squared error* (MSE) used for numerical predictions. Finally, the lab briefly introduces *regression trees* for predicting numerical variables, such as passenger ages. The instructions emphasize hands-on coding (blue questions) and journal responses (red questions) to deepen understanding."
  },
  {
    "title": "Unit 4 Lab 4G: Overview and Core Concepts of Decision Trees",
    "body": "Unit 4 Lab 4G, titled \"Growing trees,\" provides comprehensive instructions for understanding and implementing decision tree models. Participants are guided to follow along with slides, addressing questions highlighted in <span style=\"color:midnightblue;\">**blue**</span> directly on their computers, and recording answers to questions in <span style=\"color:firebrick;\">**red**</span> within their journals. This lab introduces a new predictive modeling technique, moving beyond the *linear models* learned previously. Instead of fitting lines to data, decision trees make predictions by partitioning data into *branches* based on a sequence of *yes* or *no* questions. These branches ultimately lead to *leaves*, which represent the final sorted data points used for making predictions. The initial step for this lab involves loading the `titanic` dataset, which will be used throughout the exercises to build and evaluate various tree models. This approach allows for a different, often more intuitive, way of understanding complex relationships within data."
  },
  {
    "title": "Understanding Classification Trees and Initial Model Creation in Unit 4 Lab 4G",
    "body": "In Unit 4 Lab 4G, \"Growing trees,\" students transition from *linear models* to *classification trees*. Unlike linear models that predict numerical outcomes or relationships with a line, a *classification tree* is designed specifically to predict which *category* a categorical variable belongs to based on other variables in the dataset. This lab begins by instructing users to load the `titanic` dataset, a crucial first step for all subsequent analyses. The core function for creating these trees is `tree()`, which has a syntax remarkably similar to the `lm()` function used for linear models. For the initial exercise, students are tasked with writing and running code to construct their very first classification tree, named `tree1`. This `tree1` model will predict whether a passenger `survived` the Titanic solely based on their `sex`. A key question posed early in the lab challenges students to consider why a *linear model* would not be appropriate for predicting a binary categorical outcome like `survived` based on `sex`, highlighting the fundamental difference in application between linear and classification models."
  },
  {
    "title": "Constructing and Visualizing `tree1` for Titanic Survival Prediction",
    "body": "Unit 4 Lab 4G delves into building and interpreting decision trees. The first practical task involves creating `tree1`, a *classification tree* aimed at predicting `survived` status on the Titanic based on `sex`. The `tree()` function is central to this, and its usage mirrors the `lm()` function for linear models. After assigning the model to the name `tree1`, the lab guides users to visualize and interpret this model using the `treeplot()` function. This visualization is critical for understanding how the tree makes predictions. Students are specifically asked to identify and record the labels of the two *branches* and the two *leaves* displayed in the `treeplot`. Furthermore, they must interpret the plot to determine which `sex` the model predicts will survive, where the plot indicates the total number of people sorted into each *leaf*, and how to identify the number of people *incorrectly* sorted within each leaf. This hands-on interpretation reinforces the mechanism of how classification trees sort data through *yes* or *no* questions to arrive at predictions."
  },
  {
    "title": "Building More Complex Classification Trees with Multiple Predictors in Unit 4 Lab 4G",
    "body": "Expanding on the basic *classification tree* in Unit 4 Lab 4G, the exercises progress to creating more complex models by incorporating multiple predictor variables. Following the creation of `tree1` which used only `sex`, the lab instructs users to build a second model, `tree2`. This `tree2` will predict whether a person `survived` the Titanic using a richer set of variables: `sex`, `age`, `class`, and where they `embarked`. The process of creating `tree2` with the `tree()` function is analogous to how multiple variables are included in a linear model using `lm()`. After creating `tree2`, students are directed to generate a `treeplot` for this more intricate model. Critical thinking is then applied to interpret `tree2`'s predictions, such as determining if a specific passenger (e.g., Mrs. Baxter, a 50-year-old female, 1st class from Cherbourg) is predicted to survive. The lab also prompts students to identify any variable from the chosen set that the `tree2` model ultimately did not utilize in its decision-making process, showcasing how the algorithm prunes less impactful predictors."
  },
  {
    "title": "Customizing Decision Tree Complexity with `cp` and `minsplit` in Unit 4 Lab 4G",
    "body": "Unit 4 Lab 4G explores how to control the inherent complexity of decision tree models. By default, the `tree()` function is designed to produce a *tree model* that strikes a balance between good predictive performance and a manageable number of branches, avoiding overfitting. However, the lab teaches that users can explicitly adjust the complexity of their trees. This is achieved by modifying specific parameters within the `tree()` function. The *complexity parameter*, `cp`, which defaults to `0.01`, can be decreased to allow for more complex trees with more splits. Another key parameter is `minsplit`, which defines the minimum number of observations required in a leaf before the tree attempts to split it into a new branch. The default value for `minsplit` is `20`. To demonstrate these concepts, students are instructed to create a third classification tree, `tree3`, using the same set of predictor variables as `tree2`. However, for `tree3`, the `cp` argument is set to `0.005` and `minsplit` to `10`. This exercise then requires students to compare `tree3` with `tree2` to identify how these parameter changes have altered the structure and complexity of the resulting tree."
  },
  {
    "title": "Making Predictions and Preparing for Model Evaluation in Unit 4 Lab 4G",
    "body": "Unit 4 Lab 4G introduces the crucial step of evaluating how well *classification trees* perform on new, unseen data, a process similar to how *linear models* are assessed. This evaluation is often done through *cross-validation*. The first step in this process is to generate predictions from the trained model on a separate test dataset. For this purpose, students are instructed to load the `titanic_test` data using the `data()` function. Once the test data is loaded, the lab guides users to use the `predict()` function to forecast whether passengers in `titanic_test` `survived` or not, utilizing the previously created `tree1` model. A vital detail highlighted is the argument `type = \"class\"` within the `predict()` function, which explicitly tells the function that the model is predicting a *categorical variable* rather than a numerical one. This ensures that the output of the prediction is in the correct categorical format for subsequent performance measurement. Students complete code snippets to correctly apply `predict()` and store these predictions within the `titanic_test` dataset."
  },
  {
    "title": "Measuring Performance of Classification Trees: The Misclassification Rate in Unit 4 Lab 4G",
    "body": "In Unit 4 Lab 4G, after making predictions on the `titanic_test` data, the focus shifts to quantitatively measuring the performance of *classification trees*. For models that predict *categorical variables*, such as survival on the Titanic, the standard metric is the *misclassification rate* (MCR). This is analogous to how the *mean squared error* (MSE) is used to describe the performance of models predicting numerical variables. The MCR is formally defined as the number of instances where a prediction assigns an observation to one category, but its actual outcome belongs to a different category. To facilitate understanding, the lab directs students to run a command that allows for a side-by-side visual comparison of the `survived` (actual outcome) and `prediction` (model's forecast) columns in the `titanic_test` dataset. This visual inspection helps in identifying individual misclassifications, and students are asked to pinpoint the location of the very first misclassification observed, laying the groundwork for calculating the overall MCR."
  },
  {
    "title": "Implementing a Function to Calculate Misclassification Rate in Unit 4 Lab 4G",
    "body": "To systematically quantify the performance of *classification trees* in Unit 4 Lab 4G, a function for calculating the *misclassification rate* (MCR) is introduced. The MCR serves as a critical metric for evaluating categorical predictions, similar to how *mean squared error* is used for numerical predictions. The lab guides students through creating a custom function, `calc_mcr`, which compares the actual outcomes with the predicted outcomes. The \"not equal to\" operator (`!=`) is highlighted as essential for this comparison, as `sum(actual != predicted)` will efficiently count every instance where the model's prediction does not match the true value. The complete formula for `calc_mcr` involves summing these discrepancies and then dividing by the total number of observations (`length(actual)`) to yield a proportion. After defining this function, students are instructed to run a command using `summarize()` on the `titanic_test` data to apply `calc_mcr` to the `survived` and `prediction` columns, thereby computing the overall misclassification rate for `tree1` on the test data. This practical implementation solidifies the understanding of MCR."
  },
  {
    "title": "Analyzing Misclassification Rate and the Influence of Tree Complexity in Unit 4 Lab 4G",
    "body": "Unit 4 Lab 4G culminates with a deeper analysis of the *misclassification rate* (MCR) and its relationship to tree complexity. Students are first challenged to articulate the definition of MCR in their own words, reinforcing their understanding of this crucial metric for *classification trees*. Following this, a comparative analysis is performed where students must determine which of the initial models (`tree1`, `tree2`, or `tree3`) achieved the lowest MCR on the `titanic_test` data. This comparison highlights the practical implications of including more variables (`tree2`) and adjusting complexity parameters (`tree3`). To further explore complexity, the lab instructs the creation of a fourth model, `tree4`, using the same variables as `tree2` but with an even smaller *complexity parameter* (`cp = 0.0001`). This intentionally creates a much more complex tree, prompting a critical question: does a more complex *classification tree* always lead to better predictions? This question encourages reflection on the potential for overfitting and the trade-offs involved in model complexity, demonstrating that increased complexity does not inherently guarantee improved generalization to unseen data."
  },
  {
    "title": "Introduction to Regression Trees and Holistic Model Performance Evaluation in Unit 4 Lab 4G",
    "body": "Unit 4 Lab 4G extends beyond *classification trees* to briefly introduce *regression trees*, which are a type of tree model specifically designed to predict a *numerical variable*, as opposed to the categorical variables handled by classification trees. As a final exercise, students are tasked with writing and running code to create a *regression tree* model to predict the `age` of Titanic passengers and subsequently calculate its *mean squared error* (MSE), the standard metric for evaluating numerical predictions. This reinforces the distinction between MCR for categorical predictions and MSE for numerical predictions. The lab also notes that *plots of regression trees* can often be significantly more complex and difficult to interpret compared to classification trees due to the nature of numerical splits. In summary, Unit 4 Lab 4G provides a comprehensive introduction to decision trees, covering their creation with `tree()`, visualization with `treeplot()`, customization of complexity via `cp` and `minsplit`, making predictions with `predict()` (using `type=\"class\"` for classification), and evaluating performance with the *misclassification rate* for classification tasks and *mean squared error* for regression tasks."
  }
]