[
  {
    "title": "Abstract: Lab 2I - Mastering R for Normal Distribution Analysis",
    "body": "Unit 2 Lab 2I, titled \"R's Normal Distribution Alphabet,\" provides a comprehensive guide to understanding and applying normal distributions using R. The lab's primary objectives include teaching how to simulate random data draws from a normal distribution using the `rnorm` function and how to calculate probabilities associated with normal distributions using `pnorm` and `qnorm`. Participants will begin by loading the `titanic` dataset and conducting a simulation where the `survival` status is shuffled 500 times to create a distribution of mean age differences between survivors and non-survivors. This shuffled distribution is then analyzed for normality, its center, and spread. Once normality is verified, a normal model is employed to estimate the probability of observing differences more extreme than the actual data. The lab progresses to explain how to calculate probabilities for both smaller and larger observed differences using the `pnorm` function and its complement. Furthermore, it introduces `qnorm` for determining the quantity corresponding to a given probability, effectively reversing the `pnorm` operation. The lab culminates with independent investigations using either the `titanic` or `cdc` datasets, encouraging participants to apply the learned R functions and normal modeling techniques to explore questions about age differences between genders or height disparities, and to justify claims based on statistical evidence. Throughout the lab, blue-colored questions are to be completed on the computer, while red-colored questions require detailed journal entries, fostering both practical R skills and conceptual understanding."
  },
  {
    "title": "Unit 2 Lab 2I: Exploring Normal Distributions and R Functions",
    "body": "Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" is designed to deepen understanding of normal distributions and their practical application within the R programming environment. Building upon previous lab experiences where participants learned to overlay normal curves on histograms to assess data distribution, this lab expands capabilities significantly. The prior lab also highlighted that calculating the mean of random shuffles consistently produces differences that are normally distributed, a fundamental concept revisited here. This lab specifically aims to teach two critical skills: first, how to simulate random draws from a normal distribution using dedicated R functions like `rnorm`, which allows for generating hypothetical datasets based on specified parameters (mean and standard deviation). Second, it focuses on calculating probabilities associated with normal distributions using functions such as `pnorm`, which determines the cumulative probability up to a certain value, and `qnorm`, which finds the value corresponding to a given cumulative probability. These skills are essential for statistical inference, enabling participants to model real-world phenomena and make probability-based conclusions. The lab includes hands-on exercises, requiring participants to execute code for \"blue\" questions and record detailed explanations for \"red\" questions in a journal, ensuring a comprehensive learning experience that combines coding practice with conceptual understanding."
  },
  {
    "title": "Setting Up Unit 2 Lab 2I: Titanic Data and Shuffled Survival Simulations",
    "body": "The initial phase of Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" involves a crucial setup process using the `titanic` dataset to simulate age differences. The first step requires participants to load the `titanic` data into their R environment. Following this, the core task is to simulate 500 random shuffles of the `survival` status within the `titanic` data. For each of these 500 shuffled iterations, the mean age of people in the data is calculated. This simulation process is designed to generate a distribution of mean age differences under the null hypothesis (i.e., if survival status had no real effect on age distribution, other than by chance). The results of these 500 shuffles, specifically the calculated mean ages, are then to be assigned to a new R object named `shfls`. This `shfls` object will serve as the foundation for subsequent analysis, allowing us to examine the properties of these simulated differences. This step directly addresses the objective of understanding how mean differences from random shuffles tend to be normally distributed, as previously encountered in earlier labs. This hands-on exercise emphasizes the importance of data manipulation and simulation in R, laying the groundwork for probability calculations and normal model applications."
  },
  {
    "title": "Deriving and Describing Simulated Age Differences in Unit 2 Lab 2I",
    "body": "Continuing from the initial setup in Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" after generating the `shfls` dataset through 500 random shuffles of `titanic` survival status, the next critical step involves calculating the specific differences in mean age. Participants are instructed to write and run R code using the `mutate` function to add a new variable, named `diff`, to the `shfls` dataset. This `diff` variable is defined as the mean age of those who \"survived\" in each shuffle minus the mean age of those who \"died\" in that same shuffle. This calculation quantifies the simulated age disparity for each of the 500 random scenarios. Following the creation of the `diff` variable, the lab requires calculating its overall `mean` and `standard deviation`. These two crucial descriptive statistics, representing the center and spread of the simulated difference distribution, are then to be assigned to the R objects `diff_mean` and `diff_sd`, respectively. These values are fundamental for subsequent steps, as they define the parameters of the normal model that will be used to analyze the simulated data and compare it against the actual observed difference. This process reinforces R's data manipulation capabilities and the importance of summarizing simulated data."
  },
  {
    "title": "Verifying Normality and Comparing Actual vs. Simulated Differences in Unit 2 Lab 2I",
    "body": "A crucial analytical step in Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" after calculating the `diff` variable (representing the mean age difference from 500 shuffles of `titanic` survival status) and its `diff_mean` and `diff_sd`, is to verify if this `diff` distribution is approximately normally distributed. This verification is vital because the subsequent application of a normal model relies on this assumption. Participants are asked to explain how they determined the distribution's closeness to normal and to describe its center (`diff_mean`) and spread (`diff_sd`). Typically, visual inspection (e.g., a histogram with a normal curve overlay, as in the previous lab) and statistical tests can be used for this assessment. Understanding the distribution's characteristics is key to interpreting the simulated results. Furthermore, to provide a basis for comparison, participants must compute the mean difference in age between the *actual* survivors and the *actual* non-survivors from the original `titanic` data. This \"actual difference\" serves as the observed value against which the simulated distribution and the normal model will be compared, allowing us to determine the likelihood of observing such a difference purely by chance. This section bridges the simulated environment with real-world data observation."
  },
  {
    "title": "Applying the Normal Model and Calculating \"Smaller\" Probabilities with `pnorm` in Lab 2I",
    "body": "Once the `diff` variable's distribution of mean age differences (derived from 500 shuffles of `titanic` survival status) has been verified as approximately normally distributed in Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" the normal model can be confidently employed. This model allows for estimating the probability of observing differences that are even more extreme than the *actual* difference computed from the `titanic` data. A key exercise involves drawing a sketch of a normal curve. On this sketch, participants are instructed to label the mean age difference derived from their shuffles (`diff_mean`) and the actual age difference observed between survivors and non-survivors. Crucially, they must then shade the area under the curve that represents probabilities *smaller* than the actual difference. This visual representation aids in understanding the concept of cumulative probability. To quantify this, the `pnorm` function in R is introduced. `pnorm` calculates the cumulative probability for a given quantity in a normal distribution. Participants are tasked with filling in the blanks of `pnorm(____, mean = diff_mean, sd = ____)` to calculate the probability of an even smaller difference occurring than the actual difference, using `diff_mean` and `diff_sd` as the parameters of the normal model. This function is fundamental for hypothesis testing and determining the statistical significance of observed outcomes."
  },
  {
    "title": "Calculating \"Larger\" Probabilities and Interpreting `pnorm` Results in Unit 2 Lab 2I",
    "body": "In Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" the `pnorm` function is central to calculating probabilities within a normal distribution. The probability derived from `pnorm(actual_difference, mean = diff_mean, sd = diff_sd)`, as discussed in the previous section, represents an estimate of how often we would expect to see a difference *smaller* than the actual observed difference purely by chance. This cumulative probability is crucial for evaluating hypotheses. Expanding on this, the lab also teaches how to calculate the probability of observing a difference that is *larger* than the one actually observed. Since `pnorm` provides the cumulative probability up to a certain point (i.e., the probability of being less than or equal to that point), the probability of being *greater* than that point is simply `1` minus the cumulative probability. Participants are asked to complete the R code `1 - pnorm(____, mean = diff_mean, sd = ____)` to calculate this \"larger\" probability. This complementary calculation is equally important for a complete statistical assessment, allowing for two-tailed tests or for understanding the likelihood of extreme positive deviations. This section solidifies the understanding of how `pnorm` operates and how its results can be interpreted to make comprehensive statements about the probability of observed differences."
  },
  {
    "title": "Generating Synthetic Data: Simulating Normal Draws Using `rnorm` in Unit 2 Lab 2I",
    "body": "Beyond calculating probabilities, Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" introduces the powerful `rnorm` function, which allows users to simulate random draws from a normal distribution. This capability is invaluable for creating synthetic datasets, performing Monte Carlo simulations, or understanding how variability manifests in normally distributed data. The `rnorm` function requires three key arguments: `n` (the number of random values to generate), `mean` (the mean of the normal distribution), and `sd` (the standard deviation of the normal distribution). To illustrate its use, participants are given an exercise to simulate 100 heights of randomly chosen men. Assuming a mean height of 67 inches and a standard deviation of 3 inches, they must fill in the blanks in the code: `draws <- rnorm(____, mean = ____, sd = ____)`. After generating these simulated heights, the lab instructs participants to plot them using a histogram. This visualization, by filling in `histogram(draws, fit = ____)`, helps to visually confirm that the simulated data indeed approximates a normal distribution with the specified parameters. This practical application of `rnorm` is a core skill for generating data that adheres to a normal model, crucial for various statistical analyses and simulations."
  },
  {
    "title": "Determining Quantities from Probabilities: Mastering `qnorm` in Unit 2 Lab 2I",
    "body": "Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" introduces `qnorm`, a function that performs the inverse operation of `pnorm`. While `pnorm` takes a quantity and returns a cumulative probability, `qnorm` takes a probability and returns the corresponding quantity (or quantile) in a normal distribution. This is why `pnorm` is sometimes referred to as \"P\" norm (for Probability) and `qnorm` as \"Q\" norm (for Quantile). Understanding `qnorm` is essential for identifying specific thresholds or cut-off points within a distribution, such as finding the value that marks the bottom X% or top Y% of data. An illustrative example in the lab asks: \"How tall can a man be and still be in the shortest 25% of heights if the mean height is 67 inches with a standard deviation of 3 inches?\" Participants must fill in the blanks for `qnorm(____, mean = ____, sd = ____)` to solve this. This function requires the desired probability (as a decimal, e.g., 0.25 for 25%), the mean, and the standard deviation of the distribution. `qnorm` is incredibly useful for setting benchmarks, interpreting percentiles, and understanding the range of values associated with certain probability intervals in a normal model."
  },
  {
    "title": "On Your Own: Analyzing Age Differences Between Genders on the Titanic in Unit 2 Lab 2I",
    "body": "As a culminating activity in Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" participants are challenged to conduct independent statistical investigations. One such investigation uses the `titanic` dataset to explore whether women on the Titanic were typically younger than men. This task (Question 12) requires a comprehensive application of the techniques learned throughout the lab. Participants must write and run R code to answer this question by employing three key methods:\n1.  **Histogram:** Visualizing the age distribution for men and women separately, potentially with a normal curve overlay, to gain initial insights into their central tendency and spread.\n2.  **500 Random Shuffles:** Simulating the difference in mean age between men and women under the assumption of no true difference (a null hypothesis). This involves shuffling gender status and recalculating mean age differences multiple times to build a sampling distribution of differences, similar to the initial `survival` shuffle exercise.\n3.  **Normal Model:** Using the parameters (mean and standard deviation) of the shuffled differences to construct a normal model. This model will then be used, likely with `pnorm`, to calculate the probability of observing the *actual* mean age difference between men and women (from the original `titanic` data) purely by chance. This allows for a robust statistical conclusion regarding the age disparity between genders on the Titanic. This exercise reinforces the entire analytical pipeline for comparing two groups using simulation and normal models."
  },
  {
    "title": "On Your Own: Exploring Male-Female Height Differences in CDC Data and Extreme Probabilities in Unit 2 Lab 2I",
    "body": "Another independent investigation offered in Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" uses the `cdc` dataset to delve into gender-based height differences. Specifically, Question 13 challenges participants to determine how much taller the typical male would have to be than the typical female for this difference to occur in the upper 1% purely by chance. This advanced application requires a nuanced understanding of simulation and the normal model. The process involves:\n1.  **500 Random Shuffles:** Similar to previous exercises, participants would shuffle gender labels within the `cdc` dataset and, for each shuffle, calculate the mean height difference between \"males\" and \"females.\" This generates a null distribution of height differences expected by random chance.\n2.  **Normal Model:** The `mean` and `standard deviation` of this shuffled distribution are then used to define a normal model.\n3.  **`qnorm` Application:** To find the height difference that falls into the upper 1% by chance, the `qnorm` function would be applied. Given that `qnorm` calculates the quantity for a specified cumulative probability, participants would typically use a probability of `0.99` (since the upper 1% means 99% of values are below it) along with the mean and standard deviation from their shuffled distribution. This exercise directly leverages the `qnorm` function to identify a specific threshold value for an extreme probability, moving beyond simply calculating existing probabilities. It demonstrates how to establish a benchmark for statistical significance."
  },
  {
    "title": "On Your Own: Justifying Gender Height Claims Using Normal Models in Unit 2 Lab 2I",
    "body": "The final independent investigation in Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" builds directly upon the previous question concerning height differences in the `cdc` dataset. After determining the height difference that would be in the upper 1% by chance alone (Question 13), Question 14 asks: \"How can we use this value to justify the claim that the average Male in our data is taller than the average Female?\" This question focuses on the crucial step of interpreting statistical results to make a justified claim. The justification involves comparing the *actual* observed mean height difference between males and females in the original `cdc` data to the `qnorm`-derived threshold for the upper 1% by chance. If the *actual* observed difference is significantly greater than this 1% threshold, it provides strong evidence against the null hypothesis (that there's no real difference) and supports the claim that males are, on average, taller than females, and that this difference is unlikely to have occurred by random chance. This exercise emphasizes the importance of statistical inference, where simulated distributions and normal models (using functions like `rnorm`, `pnorm`, and `qnorm`) are used to evaluate the likelihood of observed phenomena and to provide a quantitative basis for claims, moving from data analysis to informed conclusions."
  },
  {
    "title": "Summary of Essential R Functions: `rnorm`, `pnorm`, and `qnorm` in Lab 2I",
    "body": "Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" is primarily designed to familiarize participants with three cornerstone R functions for working with normal distributions: `rnorm`, `pnorm`, and `qnorm`. Each serves a distinct but complementary purpose.\n*   **`rnorm` (Random Normal):** This function is used to **simulate** or generate random numbers that follow a normal distribution. It requires specifying the number of observations (`n`), the `mean`, and the `standard deviation` of the desired distribution. For example, `rnorm(100, mean = 67, sd = 3)` would generate 100 random values from a normal distribution centered at 67 with a spread of 3. This is crucial for creating simulated datasets and understanding random variability, as demonstrated in simulating men's heights.\n*   **`pnorm` (Probability Normal):** This function calculates the **cumulative probability** for a given *quantity* (or quantile) within a normal distribution. It determines the probability that a random variable from that distribution will be less than or equal to the specified quantity. It requires the `quantity`, the `mean`, and the `standard deviation`. For instance, `pnorm(65, mean = 67, sd = 3)` gives the probability of a value being 65 or less. `pnorm` is essential for hypothesis testing, allowing us to find the likelihood of observing a particular value or something more extreme. The lab demonstrated calculating both \"smaller\" and \"larger\" probabilities (using `1 - pnorm`).\n*   **`qnorm` (Quantile Normal):** The inverse of `pnorm`, this function calculates the **quantity** (or value) that corresponds to a given *cumulative probability*. It requires the `probability` (as a decimal), the `mean`, and the `standard deviation`. For example, `qnorm(0.25, mean = 67, sd = 3)` would return the height that marks the shortest 25% of men. `qnorm` is vital for identifying thresholds, percentiles, or critical values within a normal distribution, as used in the \"On Your Own\" sections for determining extreme height differences. These three functions collectively provide a robust toolkit for comprehensively analyzing and interacting with normally distributed data in R."
  },
  {
    "title": "Comprehensive Workflow: From Data Loading to Normal Model Application in Unit 2 Lab 2I",
    "body": "Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" outlines a clear, iterative workflow for applying normal models to analyze data, particularly focusing on differences between groups. This workflow is critical for investigating research questions using simulation-based approaches.\n1.  **Data Acquisition and Preparation:** The process begins by loading the relevant dataset, such as the `titanic` data. The initial steps involve preparing this data for simulation, which, in this lab, means focusing on specific variables like `age` and `survival` status.\n2.  **Simulation of Null Distribution:** A cornerstone of this lab is the simulation of random shuffles (e.g., 500 times) of a key categorical variable (like `survival` status or `gender`). For each shuffle, a statistic of interest (e.g., mean age difference) is calculated. This creates a *sampling distribution under the null hypothesis* (e.g., no actual difference due to survival status). The results are then stored, such as in the `shfls` object.\n3.  **Calculation of Difference Statistics:** From the simulated `shfls` data, a specific `diff` variable is derived, representing the difference in means (e.g., `mean age survivors - mean age non-survivors`). Descriptive statistics, particularly the `mean` (`diff_mean`) and `standard deviation` (`diff_sd`) of this `diff` variable, are computed to characterize the simulated distribution.\n4.  **Normality Verification:** Before proceeding with a normal model, it's crucial to verify that the simulated `diff` variable is approximately normally distributed. This can be done visually with histograms and Q-Q plots, or statistically. Describing the center and spread is part of this assessment.\n5.  **Actual Observation Calculation:** The true, observed difference in the statistic of interest (e.g., mean age difference between actual survivors and non-survivors) from the original, unshuffled data is calculated. This value serves as the point of comparison for the simulated distribution.\n6.  **Normal Model Application:** Once normality is confirmed, the `diff_mean` and `diff_sd` are used as parameters for a normal model. Functions like `pnorm` are then applied to calculate the probability of observing a difference as extreme or more extreme than the actual observed difference, assuming the null hypothesis is true. `qnorm` can be used to determine critical values for certain probability thresholds.\n7.  **Interpretation and Justification:** The final step involves interpreting these probabilities or critical values to draw conclusions and justify claims about the initial research question. This systematic approach ensures robust statistical inference based on simulation and normal theory."
  },
  {
    "title": "Interpreting Statistical Significance: Drawing Conclusions from Normal Models in Unit 2 Lab 2I",
    "body": "A core objective of Unit 2 Lab 2I, \"R's Normal Distribution Alphabet,\" is not just to perform calculations with R's normal distribution functions but to understand how to interpret the results for statistical inference. Once a normal model has been established using the `mean` (`diff_mean`) and `standard deviation` (`diff_sd`) from a distribution of simulated differences (e.g., 500 shuffles of `titanic` data), the calculated probabilities become critical.\n*   **P-value Interpretation:** When `pnorm` is used to calculate the probability of observing a difference *smaller* or *larger* than the *actual* observed difference, this value effectively represents a p-value. A small p-value (e.g., less than 0.05) indicates that the observed difference is unlikely to have occurred purely by chance if there were truly no underlying difference between the groups (i.e., if the null hypothesis were true). This provides evidence to reject the null hypothesis.\n*   **Normal Model as a Null Distribution:** The normal model, parametrized by `diff_mean` and `diff_sd` from the shuffled data, serves as a representation of what differences we would expect to see if only random chance were at play. By comparing the actual observed difference to this null distribution, we can gauge its extremity.\n*   **Visual Interpretation:** The instruction to sketch a normal curve, label the `diff_mean` and the actual difference, and shade areas (e.g., *smaller* than the actual difference) is fundamental for visually grasping what these probabilities mean. It connects the abstract numerical output of `pnorm` to a tangible representation of likelihood.\n*   **Quantiles for Benchmarking:** The `qnorm` function further enhances interpretation by allowing us to determine specific thresholds. For example, finding the height difference that falls into the upper 1% by chance helps establish a benchmark for what constitutes an \"extremely large\" difference under the null hypothesis. If the actual difference surpasses this benchmark, it strengthens the case for a genuine effect.\n*   **Justifying Claims:** Ultimately, these tools empower participants to move beyond mere data description to justified claims. By quantifying the likelihood of observed outcomes under random conditions, they can confidently state whether an observed difference is statistically significant and thus, likely represents a real underlying effect rather than random variation. This is exemplified in the `cdc` data task where the calculated values are used to justify claims about male vs. female height."
  }
]